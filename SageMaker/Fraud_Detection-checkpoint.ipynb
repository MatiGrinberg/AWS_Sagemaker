{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612dfbd4-8149-4c71-94a9-415e68b3d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "import pathlib\n",
    "import io\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput, \n",
    "    ProcessingOutput, \n",
    "    ScriptProcessor\n",
    ")\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import (\n",
    "    ProcessingStep, \n",
    "    TrainingStep, \n",
    "    CreateModelStep\n",
    ")\n",
    "from sagemaker.workflow.check_job_config import CheckJobConfig\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger, \n",
    "    ParameterFloat, \n",
    "    ParameterString, \n",
    "    ParameterBoolean\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.clarify_check_step import (\n",
    "    ModelBiasCheckConfig, \n",
    "    ClarifyCheckStep, \n",
    "    ModelExplainabilityCheckConfig\n",
    ")\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    "    LambdaOutput,\n",
    "    LambdaOutputTypeEnum,\n",
    ")\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.model_metrics import (\n",
    "    MetricsSource, \n",
    "    ModelMetrics, \n",
    "    FileSource\n",
    ")\n",
    "from sagemaker.drift_check_baselines import DriftCheckBaselines\n",
    "\n",
    "from sagemaker.image_uris import retrieve\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd224c-ae44-4661-902d-d0528c364c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate AWS services session and client objects\n",
    "sess = sagemaker.Session()\n",
    "write_bucket = sess.default_bucket()\n",
    "write_prefix = \"fraud-detect-demo\"\n",
    "\n",
    "region = sess.boto_region_name\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "sm_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Fetch SageMaker execution role\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "\n",
    "# S3 locations used for parameterizing the notebook run\n",
    "read_bucket = \"sagemaker-sample-files\"\n",
    "read_prefix = \"datasets/tabular/synthetic_automobile_claims\" \n",
    "\n",
    "# S3 location where raw data to be fetched from\n",
    "raw_data_key = f\"s3://{read_bucket}/{read_prefix}\"\n",
    "\n",
    "# S3 location where processed data to be uploaded\n",
    "processed_data_key = f\"{write_prefix}/processed\"\n",
    "\n",
    "# S3 location where train data to be uploaded\n",
    "train_data_key = f\"{write_prefix}/train\"\n",
    "\n",
    "# S3 location where validation data to be uploaded\n",
    "validation_data_key = f\"{write_prefix}/validation\"\n",
    "\n",
    "# S3 location where test data to be uploaded\n",
    "test_data_key = f\"{write_prefix}/test\"\n",
    "\n",
    "\n",
    "# Full S3 paths\n",
    "claims_data_uri = f\"{raw_data_key}/claims.csv\"\n",
    "customers_data_uri = f\"{raw_data_key}/customers.csv\"\n",
    "output_data_uri = f\"s3://{write_bucket}/{write_prefix}/\"\n",
    "scripts_uri = f\"s3://{write_bucket}/{write_prefix}/scripts\"\n",
    "estimator_output_uri = f\"s3://{write_bucket}/{write_prefix}/training_jobs\"\n",
    "processing_output_uri = f\"s3://{write_bucket}/{write_prefix}/processing_jobs\"\n",
    "model_eval_output_uri = f\"s3://{write_bucket}/{write_prefix}/model_eval\"\n",
    "clarify_bias_config_output_uri = f\"s3://{write_bucket}/{write_prefix}/model_monitor/bias_config\"\n",
    "clarify_explainability_config_output_uri = f\"s3://{write_bucket}/{write_prefix}/model_monitor/explainability_config\"\n",
    "bias_report_output_uri = f\"s3://{write_bucket}/{write_prefix}/clarify_output/pipeline/bias\"\n",
    "explainability_report_output_uri = f\"s3://{write_bucket}/{write_prefix}/clarify_output/pipeline/explainability\"\n",
    "\n",
    "# Retrieve training image\n",
    "training_image = retrieve(framework=\"xgboost\", region=region, version=\"1.3-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eee608-4134-4a3d-b7fc-8e2fcd2bccf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set names of pipeline objects\n",
    "pipeline_name = \"FraudDetectXGBPipeline\"\n",
    "pipeline_model_name = \"fraud-detect-xgb-pipeline\"\n",
    "model_package_group_name = \"fraud-detect-xgb-model-group\"\n",
    "base_job_name_prefix = \"fraud-detect\"\n",
    "endpoint_config_name = f\"{pipeline_model_name}-endpoint-config\"\n",
    "endpoint_name = f\"{pipeline_model_name}-endpoint\"\n",
    "\n",
    "# Set data parameters\n",
    "target_col = \"fraud\"\n",
    "\n",
    "# Set instance types and counts\n",
    "process_instance_type = \"ml.c5.xlarge\"\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "predictor_instance_count = 1\n",
    "predictor_instance_type = \"ml.m4.xlarge\"\n",
    "clarify_instance_count = 1\n",
    "clarify_instance_type = \"ml.m4.xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d2faf-2095-4de5-afb6-6182da41d358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up pipeline input parameters\n",
    "\n",
    "# Set processing instance type\n",
    "process_instance_type_param = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=process_instance_type,\n",
    ")\n",
    "\n",
    "# Set training instance type\n",
    "train_instance_type_param = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=train_instance_type,\n",
    ")\n",
    "\n",
    "# Set training instance count\n",
    "train_instance_count_param = ParameterInteger(\n",
    "    name=\"TrainingInstanceCount\",\n",
    "    default_value=train_instance_count\n",
    ")\n",
    "\n",
    "# Set deployment instance type\n",
    "deploy_instance_type_param = ParameterString(\n",
    "    name=\"DeployInstanceType\",\n",
    "    default_value=predictor_instance_type,\n",
    ")\n",
    "\n",
    "# Set deployment instance count\n",
    "deploy_instance_count_param = ParameterInteger(\n",
    "    name=\"DeployInstanceCount\",\n",
    "    default_value=predictor_instance_count\n",
    ")\n",
    "\n",
    "# Set Clarify check instance type\n",
    "clarify_instance_type_param = ParameterString(\n",
    "    name=\"ClarifyInstanceType\",\n",
    "    default_value=clarify_instance_type,\n",
    ")\n",
    "\n",
    "# Set model bias check params\n",
    "skip_check_model_bias_param = ParameterBoolean(\n",
    "    name=\"SkipModelBiasCheck\", \n",
    "    default_value=False\n",
    ")\n",
    "\n",
    "register_new_baseline_model_bias_param = ParameterBoolean(\n",
    "    name=\"RegisterNewModelBiasBaseline\",\n",
    "    default_value=False\n",
    ")\n",
    "\n",
    "supplied_baseline_constraints_model_bias_param = ParameterString(\n",
    "    name=\"ModelBiasSuppliedBaselineConstraints\", \n",
    "    default_value=\"\"\n",
    ")\n",
    "\n",
    "# Set model explainability check params\n",
    "skip_check_model_explainability_param = ParameterBoolean(\n",
    "    name=\"SkipModelExplainabilityCheck\", \n",
    "    default_value=False\n",
    ")\n",
    "\n",
    "register_new_baseline_model_explainability_param = ParameterBoolean(\n",
    "    name=\"RegisterNewModelExplainabilityBaseline\",\n",
    "    default_value=False\n",
    ")\n",
    "\n",
    "supplied_baseline_constraints_model_explainability_param = ParameterString(\n",
    "    name=\"ModelExplainabilitySuppliedBaselineConstraints\", \n",
    "    default_value=\"\"\n",
    ")\n",
    "\n",
    "# Set model approval param\n",
    "model_approval_status_param = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"Approved\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb2157-ebcf-410d-bda3-510cfc31af9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import pathlib\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-ratio\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--validation-ratio\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--test-ratio\", type=float, default=0.1)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    logger.info(\"Received arguments {}\".format(args))\n",
    "    \n",
    "    # Set local path prefix in the processing container\n",
    "    local_dir = \"/opt/ml/processing\"    \n",
    "    \n",
    "    input_data_path_claims = os.path.join(\"/opt/ml/processing/claims\", \"claims.csv\")\n",
    "    input_data_path_customers = os.path.join(\"/opt/ml/processing/customers\", \"customers.csv\")\n",
    "    \n",
    "    logger.info(\"Reading claims data from {}\".format(input_data_path_claims))\n",
    "    df_claims = pd.read_csv(input_data_path_claims)\n",
    "    \n",
    "    logger.info(\"Reading customers data from {}\".format(input_data_path_customers))\n",
    "    df_customers = pd.read_csv(input_data_path_customers)\n",
    "    \n",
    "    logger.debug(\"Formatting column names.\")\n",
    "    # Format column names\n",
    "    df_claims = df_claims.rename({c : c.lower().strip().replace(' ', '_') for c in df_claims.columns}, axis = 1)\n",
    "    df_customers = df_customers.rename({c : c.lower().strip().replace(' ', '_') for c in df_customers.columns}, axis = 1)\n",
    "    \n",
    "    logger.debug(\"Joining datasets.\")\n",
    "    # Join datasets\n",
    "    df_data = df_claims.merge(df_customers, on = 'policy_id', how = 'left')\n",
    "\n",
    "    # Drop selected columns not required for model building\n",
    "    df_data = df_data.drop(['customer_zip'], axis = 1)\n",
    "    \n",
    "    # Select Ordinal columns\n",
    "    ordinal_cols = [\"police_report_available\", \"policy_liability\", \"customer_education\"]\n",
    "\n",
    "    # Select categorical columns and filling with na\n",
    "    cat_cols_all = list(df_data.select_dtypes('object').columns)\n",
    "    cat_cols = [c for c in cat_cols_all if c not in ordinal_cols]\n",
    "    df_data[cat_cols] = df_data[cat_cols].fillna('na')\n",
    "    \n",
    "    logger.debug(\"One-hot encoding categorical columns.\")\n",
    "    # One-hot encoding categorical columns\n",
    "    df_data = pd.get_dummies(df_data, columns = cat_cols)\n",
    "    \n",
    "    logger.debug(\"Encoding ordinal columns.\")\n",
    "    # Ordinal encoding\n",
    "    mapping = {\n",
    "               \"Yes\": \"1\",\n",
    "               \"No\": \"0\" \n",
    "              }\n",
    "    df_data['police_report_available'] = df_data['police_report_available'].map(mapping)\n",
    "    df_data['police_report_available'] = df_data['police_report_available'].astype(float)\n",
    "\n",
    "    mapping = {\n",
    "               \"15/30\": \"0\",\n",
    "               \"25/50\": \"1\", \n",
    "               \"30/60\": \"2\",\n",
    "               \"100/200\": \"3\"\n",
    "              }\n",
    "    \n",
    "    df_data['policy_liability'] = df_data['policy_liability'].map(mapping)\n",
    "    df_data['policy_liability'] = df_data['policy_liability'].astype(float)\n",
    "\n",
    "    mapping = {\n",
    "               \"Below High School\": \"0\",\n",
    "               \"High School\": \"1\", \n",
    "               \"Associate\": \"2\",\n",
    "               \"Bachelor\": \"3\",\n",
    "               \"Advanced Degree\": \"4\"\n",
    "              }\n",
    "    \n",
    "    df_data['customer_education'] = df_data['customer_education'].map(mapping)\n",
    "    df_data['customer_education'] = df_data['customer_education'].astype(float)\n",
    "    \n",
    "    df_processed = df_data.copy()\n",
    "    df_processed.columns = [c.lower() for c in df_data.columns]\n",
    "    df_processed = df_processed.drop([\"policy_id\", \"customer_gender_unkown\"], axis=1)\n",
    "    \n",
    "    # Split into train, validation, and test sets\n",
    "    train_ratio = args.train_ratio\n",
    "    val_ratio = args.validation_ratio\n",
    "    test_ratio = args.test_ratio\n",
    "    \n",
    "    logger.debug(\"Splitting data into train, validation, and test sets\")\n",
    "    \n",
    "    y = df_processed['fraud']\n",
    "    X = df_processed.drop(['fraud'], axis = 1)\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_ratio, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_ratio, random_state=42)\n",
    "\n",
    "    train_df = pd.concat([y_train, X_train], axis = 1)\n",
    "    val_df = pd.concat([y_val, X_val], axis = 1)\n",
    "    test_df = pd.concat([y_test, X_test], axis = 1)\n",
    "    dataset_df = pd.concat([y, X], axis = 1)\n",
    "    \n",
    "    logger.info(\"Train data shape after preprocessing: {}\".format(train_df.shape))\n",
    "    logger.info(\"Validation data shape after preprocessing: {}\".format(val_df.shape))\n",
    "    logger.info(\"Test data shape after preprocessing: {}\".format(test_df.shape))\n",
    "    \n",
    "    # Save processed datasets to the local paths in the processing container.\n",
    "    # SageMaker will upload the contents of these paths to S3 bucket\n",
    "    logger.debug(\"Writing processed datasets to container local path.\")\n",
    "    train_output_path = os.path.join(f\"{local_dir}/train\", \"train.csv\")\n",
    "    validation_output_path = os.path.join(f\"{local_dir}/val\", \"validation.csv\")\n",
    "    test_output_path = os.path.join(f\"{local_dir}/test\", \"test.csv\")\n",
    "    full_processed_output_path = os.path.join(f\"{local_dir}/full\", \"dataset.csv\")\n",
    "\n",
    "    logger.info(\"Saving train data to {}\".format(train_output_path))\n",
    "    train_df.to_csv(train_output_path, index=False)\n",
    "    \n",
    "    logger.info(\"Saving validation data to {}\".format(validation_output_path))\n",
    "    val_df.to_csv(validation_output_path, index=False)\n",
    "\n",
    "    logger.info(\"Saving test data to {}\".format(test_output_path))\n",
    "    test_df.to_csv(test_output_path, index=False)\n",
    "    \n",
    "    logger.info(\"Saving full processed data to {}\".format(full_processed_output_path))\n",
    "    dataset_df.to_csv(full_processed_output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c2846d-58dd-4680-b2ff-c13fd2b4d1ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "# Upload processing script to S3\n",
    "s3_client.upload_file(\n",
    "    Filename=\"preprocessing.py\", Bucket=write_bucket, Key=f\"{write_prefix}/scripts/preprocessing.py\"\n",
    ")\n",
    "\n",
    "# Define the SKLearnProcessor configuration\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type=process_instance_type,\n",
    "    base_job_name=f\"{base_job_name_prefix}-processing\",\n",
    ")\n",
    "\n",
    "# Define pipeline processing step\n",
    "process_step = ProcessingStep(\n",
    "    name=\"DataProcessing\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=claims_data_uri, destination=\"/opt/ml/processing/claims\"),\n",
    "        ProcessingInput(source=customers_data_uri, destination=\"/opt/ml/processing/customers\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(destination=f\"{processing_output_uri}/train_data\", output_name=\"train_data\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(destination=f\"{processing_output_uri}/validation_data\", output_name=\"validation_data\", source=\"/opt/ml/processing/val\"),\n",
    "        ProcessingOutput(destination=f\"{processing_output_uri}/test_data\", output_name=\"test_data\", source=\"/opt/ml/processing/test\"),\n",
    "        ProcessingOutput(destination=f\"{processing_output_uri}/processed_data\", output_name=\"processed_data\", source=\"/opt/ml/processing/full\")\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--train-ratio\", \"0.8\", \n",
    "        \"--validation-ratio\", \"0.1\",\n",
    "        \"--test-ratio\", \"0.1\"\n",
    "    ],\n",
    "    code=f\"s3://{write_bucket}/{write_prefix}/scripts/preprocessing.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7276d-c64c-4abd-8a04-291003e3b28f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile xgboost_train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters and algorithm parameters are described here\n",
    "    parser.add_argument(\"--num_round\", type=int, default=100)\n",
    "    parser.add_argument(\"--max_depth\", type=int, default=3)\n",
    "    parser.add_argument(\"--eta\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--subsample\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--colsample_bytree\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--objective\", type=str, default=\"binary:logistic\")\n",
    "    parser.add_argument(\"--eval_metric\", type=str, default=\"auc\")\n",
    "    parser.add_argument(\"--nfold\", type=int, default=3)\n",
    "    parser.add_argument(\"--early_stopping_rounds\", type=int, default=3)\n",
    "    \n",
    "\n",
    "    # SageMaker specific arguments. Defaults are set in the environment variables\n",
    "    # Set location of input training data\n",
    "    parser.add_argument(\"--train_data_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    # Set location of input validation data\n",
    "    parser.add_argument(\"--validation_data_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_VALIDATION\"))\n",
    "    # Set location where trained model will be stored. Default set by SageMaker, /opt/ml/model\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    # Set location where model artifacts will be stored. Default set by SageMaker, /opt/ml/output/data\n",
    "    parser.add_argument(\"--output_data_dir\", type=str, default=os.environ.get(\"SM_OUTPUT_DATA_DIR\"))\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    data_train = pd.read_csv(f\"{args.train_data_dir}/train.csv\")\n",
    "    train = data_train.drop(\"fraud\", axis=1)\n",
    "    label_train = pd.DataFrame(data_train[\"fraud\"])\n",
    "    dtrain = xgb.DMatrix(train, label=label_train)\n",
    "    \n",
    "    \n",
    "    data_validation = pd.read_csv(f\"{args.validation_data_dir}/validation.csv\")\n",
    "    validation = data_validation.drop(\"fraud\", axis=1)\n",
    "    label_validation = pd.DataFrame(data_validation[\"fraud\"])\n",
    "    dvalidation = xgb.DMatrix(validation, label=label_validation)\n",
    "    \n",
    "    # Choose XGBoost model hyperparameters\n",
    "    params = {\"max_depth\": args.max_depth,\n",
    "              \"eta\": args.eta,\n",
    "              \"objective\": args.objective,\n",
    "              \"subsample\" : args.subsample,\n",
    "              \"colsample_bytree\":args.colsample_bytree\n",
    "             }\n",
    "    \n",
    "    num_boost_round = args.num_round\n",
    "    nfold = args.nfold\n",
    "    early_stopping_rounds = args.early_stopping_rounds\n",
    "    \n",
    "    # Cross-validate train XGBoost model\n",
    "    cv_results = xgb.cv(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        nfold=nfold,\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        metrics=[\"auc\"],\n",
    "        seed=42,\n",
    "    )\n",
    "    \n",
    "    model = xgb.train(params=params, dtrain=dtrain, num_boost_round=len(cv_results))\n",
    "    \n",
    "    train_pred = model.predict(dtrain)\n",
    "    validation_pred = model.predict(dvalidation)\n",
    "    \n",
    "    train_auc = roc_auc_score(label_train, train_pred)\n",
    "    validation_auc = roc_auc_score(label_validation, validation_pred)\n",
    "    \n",
    "    print(f\"[0]#011train-auc:{train_auc:.2f}\")\n",
    "    print(f\"[0]#011validation-auc:{validation_auc:.2f}\")\n",
    "\n",
    "    metrics_data = {\"hyperparameters\" : params,\n",
    "                    \"binary_classification_metrics\": {\"validation:auc\": {\"value\": validation_auc},\n",
    "                                                      \"train:auc\": {\"value\": train_auc}\n",
    "                                                     }\n",
    "                   }\n",
    "              \n",
    "    # Save the evaluation metrics to the location specified by output_data_dir\n",
    "    metrics_location = args.output_data_dir + \"/metrics.json\"\n",
    "    \n",
    "    # Save the trained model to the location specified by model_dir\n",
    "    model_location = args.model_dir + \"/xgboost-model\"\n",
    "\n",
    "    with open(metrics_location, \"w\") as f:\n",
    "        json.dump(metrics_data, f)\n",
    "\n",
    "    with open(model_location, \"wb\") as f:\n",
    "        joblib.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9916e-8c0e-437c-a5d5-d6e72b8e062e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set XGBoost model hyperparameters \n",
    "hyperparams = {  \n",
    "\"eval_metric\" : \"auc\",\n",
    "\"objective\": \"binary:logistic\",\n",
    "\"num_round\": \"5\",\n",
    "\"max_depth\":\"5\",\n",
    "\"subsample\":\"0.75\",\n",
    "\"colsample_bytree\":\"0.75\",\n",
    "\"eta\":\"0.5\"\n",
    "}\n",
    "\n",
    "# Set XGBoost estimator\n",
    "xgb_estimator = XGBoost(\n",
    "entry_point=\"xgboost_train.py\", \n",
    "output_path=estimator_output_uri,\n",
    "code_location=estimator_output_uri,\n",
    "hyperparameters=hyperparams,\n",
    "role=sagemaker_role,\n",
    "# Fetch instance type and count from pipeline parameters\n",
    "instance_count=train_instance_count,\n",
    "instance_type=train_instance_type,\n",
    "framework_version=\"1.3-1\"\n",
    ")\n",
    "\n",
    "# Access the location where the preceding processing step saved train and validation datasets\n",
    "# Pipeline step properties can give access to outputs which can be used in succeeding steps\n",
    "s3_input_train = TrainingInput(\n",
    "s3_data=process_step.properties.ProcessingOutputConfig.Outputs[\"train_data\"].S3Output.S3Uri, \n",
    "content_type=\"csv\", \n",
    "s3_data_type=\"S3Prefix\"\n",
    ")\n",
    "s3_input_validation = TrainingInput(\n",
    "s3_data=process_step.properties.ProcessingOutputConfig.Outputs[\"validation_data\"].S3Output.S3Uri,\n",
    "content_type=\"csv\",\n",
    "s3_data_type=\"S3Prefix\"\n",
    ")\n",
    "\n",
    "# Set pipeline training step\n",
    "train_step = TrainingStep(\n",
    "name=\"XGBModelTraining\",\n",
    "estimator=xgb_estimator,\n",
    "inputs={\n",
    "\"train\":s3_input_train, # Train channel \n",
    "\"validation\": s3_input_validation # Validation channel\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6d95e7-5861-4e5b-80a5-c896769c095f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a SageMaker model\n",
    "model = sagemaker.model.Model(\n",
    "    image_uri=training_image,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sess,\n",
    "    role=sagemaker_role\n",
    ")\n",
    "\n",
    "# Specify model deployment instance type\n",
    "inputs = sagemaker.inputs.CreateModelInput(instance_type=deploy_instance_type_param)\n",
    "\n",
    "create_model_step = CreateModelStep(name=\"FraudDetModel\", model=model, inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f25fac2-b819-427e-9c2d-9c0e07d0bac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up common configuration parameters to be used across multiple steps\n",
    "check_job_config = CheckJobConfig(\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type=clarify_instance_type,\n",
    "    volume_size_in_gb=30,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "# Set up configuration of data to be used for model bias check\n",
    "model_bias_data_config = sagemaker.clarify.DataConfig(\n",
    "    # Fetch S3 location where processing step saved train data\n",
    "    s3_data_input_path=process_step.properties.ProcessingOutputConfig.Outputs[\"train_data\"].S3Output.S3Uri,\n",
    "    s3_output_path=bias_report_output_uri,\n",
    "    label=target_col,\n",
    "    dataset_type=\"text/csv\",\n",
    "    s3_analysis_config_output_path=clarify_bias_config_output_uri\n",
    ")\n",
    "\n",
    "# Set up details of the trained model to be checked for bias\n",
    "model_config = sagemaker.clarify.ModelConfig(\n",
    "    # Pull model name from model creation step\n",
    "    model_name=create_model_step.properties.ModelName,\n",
    "    instance_count=train_instance_count,\n",
    "    instance_type=train_instance_type\n",
    ")\n",
    "\n",
    "# Set up column and categories that are to be checked for bias\n",
    "model_bias_config = sagemaker.clarify.BiasConfig(\n",
    "    label_values_or_threshold=[0],\n",
    "    facet_name=\"customer_gender_female\",\n",
    "    facet_values_or_threshold=[1]\n",
    ")\n",
    "\n",
    "# Set up model predictions configuration to get binary labels from probabilities\n",
    "model_predictions_config = sagemaker.clarify.ModelPredictedLabelConfig(probability_threshold=0.5)\n",
    "\n",
    "model_bias_check_config = ModelBiasCheckConfig(\n",
    "    data_config=model_bias_data_config,\n",
    "    data_bias_config=model_bias_config,\n",
    "    model_config=model_config,\n",
    "    model_predicted_label_config=model_predictions_config,\n",
    "    methods=[\"DPPL\"]\n",
    ")\n",
    "\n",
    "# Set up pipeline model bias check step\n",
    "model_bias_check_step = ClarifyCheckStep(\n",
    "    name=\"ModelBiasCheck\",\n",
    "    clarify_check_config=model_bias_check_config,\n",
    "    check_job_config=check_job_config,\n",
    "    skip_check=skip_check_model_bias_param,\n",
    "    register_new_baseline=register_new_baseline_model_bias_param,\n",
    "    supplied_baseline_constraints=supplied_baseline_constraints_model_bias_param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c6659d-577e-4d8e-b48c-dbc35a0180df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set configuration of data to be used for model explainability check\n",
    "model_explainability_data_config = sagemaker.clarify.DataConfig(\n",
    "    # Fetch S3 location where processing step saved train data\n",
    "    s3_data_input_path=process_step.properties.ProcessingOutputConfig.Outputs[\"train_data\"].S3Output.S3Uri,\n",
    "    s3_output_path=explainability_report_output_uri,\n",
    "    label=target_col,\n",
    "    dataset_type=\"text/csv\",\n",
    "    s3_analysis_config_output_path=clarify_explainability_config_output_uri \n",
    ")\n",
    "\n",
    "# Set SHAP configuration for Clarify to compute global and local SHAP values for feature importance\n",
    "shap_config = sagemaker.clarify.SHAPConfig(\n",
    "    seed=42, \n",
    "    num_samples=100,\n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=True\n",
    ")\n",
    "\n",
    "model_explainability_config = ModelExplainabilityCheckConfig(\n",
    "    data_config=model_explainability_data_config,\n",
    "    model_config=model_config,\n",
    "    explainability_config=shap_config\n",
    ")\n",
    "\n",
    "# Set pipeline model explainability check step\n",
    "model_explainability_step = ClarifyCheckStep(\n",
    "    name=\"ModelExplainabilityCheck\",\n",
    "    clarify_check_config=model_explainability_config,\n",
    "    check_job_config=check_job_config,\n",
    "    skip_check=skip_check_model_explainability_param,\n",
    "    register_new_baseline=register_new_baseline_model_explainability_param,\n",
    "    supplied_baseline_constraints=supplied_baseline_constraints_model_explainability_param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b9d7f-84e9-4d5a-9ceb-04d76bdbc791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile evaluate.py\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "\n",
    "    logger.debug(\"Loading xgboost model.\")\n",
    "    # The name of the file should match how the model was saved in the training script\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    logger.debug(\"Reading test data.\")\n",
    "    test_local_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df_test = pd.read_csv(test_local_path)\n",
    "    \n",
    "    # Extract test set target column\n",
    "    y_test = df_test.iloc[:, 0].values\n",
    "   \n",
    "    cols_when_train = model.feature_names\n",
    "    # Extract test set feature columns\n",
    "    X = df_test[cols_when_train].copy()\n",
    "    X_test = xgb.DMatrix(X)\n",
    "\n",
    "    logger.info(\"Generating predictions for test data.\")\n",
    "    pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate model evaluation score\n",
    "    logger.debug(\"Calculating ROC-AUC score.\")\n",
    "    auc = roc_auc_score(y_test, pred)\n",
    "    metric_dict = {\n",
    "        \"classification_metrics\": {\"roc_auc\": {\"value\": auc}}\n",
    "    }\n",
    "    \n",
    "    # Save model evaluation metrics\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logger.info(\"Writing evaluation report with ROC-AUC: %f\", auc)\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(metric_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33051d07-c40c-4046-b6b6-a15d3443b807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload model evaluation script to S3\n",
    "s3_client.upload_file(\n",
    "    Filename=\"evaluate.py\", Bucket=write_bucket, Key=f\"{write_prefix}/scripts/evaluate.py\"\n",
    ")\n",
    "\n",
    "eval_processor = ScriptProcessor(\n",
    "    image_uri=training_image,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=predictor_instance_type,\n",
    "    instance_count=predictor_instance_count,\n",
    "    base_job_name=f\"{base_job_name_prefix}-model-eval\",\n",
    "    sagemaker_session=sess,\n",
    "    role=sagemaker_role,\n",
    ")\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"FraudDetEvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "\n",
    "# Set model evaluation step\n",
    "evaluation_step = ProcessingStep(\n",
    "    name=\"XGBModelEvaluate\",\n",
    "    processor=eval_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            # Fetch S3 location where train step saved model artifacts\n",
    "            source=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            # Fetch S3 location where processing step saved test data\n",
    "            source=process_step.properties.ProcessingOutputConfig.Outputs[\"test_data\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(destination=f\"{model_eval_output_uri}\", output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=f\"s3://{write_bucket}/{write_prefix}/scripts/evaluate.py\",\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe3e295-9fac-4aa4-a9fc-1467d1445c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch baseline constraints to record in model registry\n",
    "model_metrics = ModelMetrics(\n",
    "    bias_post_training=MetricsSource(\n",
    "        s3_uri=model_bias_check_step.properties.CalculatedBaselineConstraints,\n",
    "        content_type=\"application/json\"\n",
    "    ),\n",
    "    explainability=MetricsSource(\n",
    "        s3_uri=model_explainability_step.properties.CalculatedBaselineConstraints,\n",
    "        content_type=\"application/json\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fetch baselines to record in model registry for drift check\n",
    "drift_check_baselines = DriftCheckBaselines(\n",
    "    bias_post_training_constraints=MetricsSource(\n",
    "        s3_uri=model_bias_check_step.properties.BaselineUsedForDriftCheckConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    explainability_constraints=MetricsSource(\n",
    "        s3_uri=model_explainability_step.properties.BaselineUsedForDriftCheckConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    explainability_config_file=FileSource(\n",
    "        s3_uri=model_explainability_config.monitoring_analysis_config_uri,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Define register model step\n",
    "register_step = RegisterModel(\n",
    "    name=\"XGBRegisterModel\",\n",
    "    estimator=xgb_estimator,\n",
    "    # Fetching S3 location where train step saved model artifacts\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[predictor_instance_type],\n",
    "    transform_instances=[predictor_instance_type],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status_param,\n",
    "    # Registering baselines metrics that can be used for model monitoring\n",
    "    model_metrics=model_metrics,\n",
    "    drift_check_baselines=drift_check_baselines\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde327da-5817-42f4-a05e-743331ff5361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile lambda_deployer.py\n",
    "\n",
    "\"\"\"\n",
    "Lambda function creates an endpoint configuration and deploys a model to real-time endpoint. \n",
    "Required parameters for deployment are retrieved from the event object\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "    # Details of the model created in the Pipeline CreateModelStep\n",
    "    model_name = event[\"model_name\"]\n",
    "    model_package_arn = event[\"model_package_arn\"]\n",
    "    endpoint_config_name = event[\"endpoint_config_name\"]\n",
    "    endpoint_name = event[\"endpoint_name\"]\n",
    "    role = event[\"role\"]\n",
    "    instance_type = event[\"instance_type\"]\n",
    "    instance_count = event[\"instance_count\"]\n",
    "    primary_container = {\"ModelPackageName\": model_package_arn}\n",
    "\n",
    "    # Create model\n",
    "    model = sm_client.create_model(\n",
    "        ModelName=model_name,\n",
    "        PrimaryContainer=primary_container,\n",
    "        ExecutionRoleArn=role\n",
    "    )\n",
    "\n",
    "    # Create endpoint configuration\n",
    "    create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"Alltraffic\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InitialInstanceCount\": instance_count,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1\n",
    "        }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create endpoint\n",
    "    create_endpoint_response = sm_client.create_endpoint(\n",
    "        EndpointName=endpoint_name, \n",
    "        EndpointConfigName=endpoint_config_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a935cf30-38d2-408c-802e-1bb28fde5efb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The function name must contain sagemaker\n",
    "function_name = \"sagemaker-fraud-det-demo-lambda-step\"\n",
    "# Define Lambda helper class can be used to create the Lambda function required in the Lambda step\n",
    "func = Lambda(\n",
    "    function_name=function_name,\n",
    "    execution_role_arn=sagemaker_role,\n",
    "    script=\"lambda_deployer.py\",\n",
    "    handler=\"lambda_deployer.lambda_handler\",\n",
    "    timeout=600,\n",
    "    memory_size=3008,\n",
    ")\n",
    "\n",
    "# The inputs used in the lambda handler are passed through the inputs argument in the \n",
    "# LambdaStep and retrieved via the `event` object within the `lambda_handler` function\n",
    "\n",
    "lambda_deploy_step = LambdaStep(\n",
    "    name=\"LambdaStepRealTimeDeploy\",\n",
    "    lambda_func=func,\n",
    "    inputs={\n",
    "        \"model_name\": pipeline_model_name,\n",
    "        \"endpoint_config_name\": endpoint_config_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"model_package_arn\": register_step.steps[0].properties.ModelPackageArn,\n",
    "        \"role\": sagemaker_role,\n",
    "        \"instance_type\": deploy_instance_type_param,\n",
    "        \"instance_count\": deploy_instance_count_param\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a4485-e06a-41cb-ab3d-70e8c5fedf41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate model performance on test set\n",
    "cond_gte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"classification_metrics.roc_auc.value\",\n",
    "    ),\n",
    "    right=0.7, # Threshold to compare model performance against\n",
    ")\n",
    "condition_step = ConditionStep(\n",
    "    name=\"CheckFraudDetXGBEvaluation\",\n",
    "    conditions=[cond_gte],\n",
    "    if_steps=[create_model_step, model_bias_check_step, model_explainability_step, register_step, lambda_deploy_step], \n",
    "    else_steps=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b971d-b564-49d9-85ce-d79a7438fe94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the Pipeline with all component steps and parameters\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[process_instance_type_param, \n",
    "                train_instance_type_param, \n",
    "                train_instance_count_param, \n",
    "                deploy_instance_type_param,\n",
    "                deploy_instance_count_param,\n",
    "                clarify_instance_type_param,\n",
    "                skip_check_model_bias_param,\n",
    "                register_new_baseline_model_bias_param,\n",
    "                supplied_baseline_constraints_model_bias_param,\n",
    "                skip_check_model_explainability_param,\n",
    "                register_new_baseline_model_explainability_param,\n",
    "                supplied_baseline_constraints_model_explainability_param,\n",
    "                model_approval_status_param],\n",
    "    steps=[\n",
    "        process_step,\n",
    "        train_step,\n",
    "        evaluation_step,\n",
    "        condition_step\n",
    "    ],\n",
    "    sagemaker_session=sess\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b5b172-ea6b-4177-9181-c2f7d6a0943b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new or update existing Pipeline\n",
    "pipeline.upsert(role_arn=sagemaker_role)\n",
    "\n",
    "# Full Pipeline description\n",
    "pipeline_definition = json.loads(pipeline.describe()['PipelineDefinition'])\n",
    "pipeline_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19e46ed3-4802-4e73-9e50-1f181a04b4c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute Pipeline\n",
    "start_response = pipeline.start(parameters=dict(\n",
    "        SkipModelBiasCheck=True,\n",
    "        RegisterNewModelBiasBaseline=True,\n",
    "        SkipModelExplainabilityCheck=True,\n",
    "        RegisterNewModelExplainabilityBaseline=True)\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "660f52bb-4c2f-4fc8-becc-54bc81f27e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "sagemaker-us-east-1-821908415262/fraud-detect-demo/processing_jobs/test_data/test.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/s3fs/core.py:529\u001b[0m, in \u001b[0;36mS3FileSystem.info\u001b[0;34m(self, path, version_id, refresh)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 529\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mversion_id_kw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreq_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mETag\u001b[39m\u001b[38;5;124m'\u001b[39m: out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mETag\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKey\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([bucket, key]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVersionId\u001b[39m\u001b[38;5;124m'\u001b[39m: out\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVersionId\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    541\u001b[0m     }\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/s3fs/core.py:200\u001b[0m, in \u001b[0;36mS3FileSystem._call_s3\u001b[0;34m(self, method, *akwarglist, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m additional_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_s3_method_kwargs(method, \u001b[38;5;241m*\u001b[39makwarglist,\n\u001b[1;32m    199\u001b[0m                                                \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (404) when calling the HeadObject operation: Not Found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fetch test data to run predictions with the endpoint\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprocessing_output_uri\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/test_data/test.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create SageMaker Predictor from the deployed endpoint\u001b[39;00m\n\u001b[1;32m      5\u001b[0m predictor \u001b[38;5;241m=\u001b[39m sagemaker\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mPredictor(endpoint_name, \n\u001b[1;32m      6\u001b[0m                                           sagemaker_session\u001b[38;5;241m=\u001b[39msess,\n\u001b[1;32m      7\u001b[0m                                           serializer\u001b[38;5;241m=\u001b[39mCSVSerializer(),\n\u001b[1;32m      8\u001b[0m                                           deserializer\u001b[38;5;241m=\u001b[39mCSVDeserializer()\n\u001b[1;32m      9\u001b[0m                                          )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    715\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    727\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:420\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     file_obj \u001b[38;5;241m=\u001b[39m \u001b[43mfsspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsspec_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 420\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# GH 34626 Reads from Public Buckets without Credentials needs anon=True\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(err_types_to_retry_with_anon):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/core.py:141\u001b[0m, in \u001b[0;36mOpenFile.open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    134\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Materialise this as a real open file without context\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    The file should be explicitly closed to avoid enclosed file\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    been deleted; but a with-context is better style.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__enter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     closer \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mclose\n\u001b[1;32m    143\u001b[0m     fobjects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfobjects\u001b[38;5;241m.\u001b[39mcopy()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/core.py:104\u001b[0m, in \u001b[0;36mOpenFile.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    102\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 104\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfobjects \u001b[38;5;241m=\u001b[39m [f]\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/spec.py:1034\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1033\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1034\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1043\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/s3fs/core.py:375\u001b[0m, in \u001b[0;36mS3FileSystem._open\u001b[0;34m(self, path, mode, block_size, acl, version_id, fill_cache, cache_type, autocommit, requester_pays, **kwargs)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    373\u001b[0m     cache_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_cache_type\n\u001b[0;32m--> 375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mS3File\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m              \u001b[49m\u001b[43mversion_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m              \u001b[49m\u001b[43ms3_additional_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m              \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocommit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequester_pays\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequester_pays\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/s3fs/core.py:1096\u001b[0m, in \u001b[0;36mS3File.__init__\u001b[0;34m(self, s3, path, mode, block_size, acl, version_id, fill_cache, s3_additional_kwargs, autocommit, cache_type, requester_pays)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms3_additional_kwargs \u001b[38;5;241m=\u001b[39m s3_additional_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreq_kw \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRequestPayer\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequester\u001b[39m\u001b[38;5;124m'\u001b[39m} \u001b[38;5;28;01mif\u001b[39;00m requester_pays \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m-> 1096\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocommit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mcache_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs  \u001b[38;5;66;03m# compatibility\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwritable():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/spec.py:1382\u001b[0m, in \u001b[0;36mAbstractBufferedFile.__init__\u001b[0;34m(self, fs, path, mode, block_size, autocommit, cache_type, cache_options, size, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m   1381\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1382\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetails\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m caches[cache_type](\n\u001b[1;32m   1384\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_range, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcache_options\n\u001b[1;32m   1385\u001b[0m     )\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/spec.py:1395\u001b[0m, in \u001b[0;36mAbstractBufferedFile.details\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetails\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/s3fs/core.py:546\u001b[0m, in \u001b[0;36mS3FileSystem.info\u001b[0;34m(self, path, version_id, refresh)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# This could have failed since the thing we are looking for is a prefix.\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ee, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m--> 546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mS3FileSystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ee\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/spec.py:615\u001b[0m, in \u001b[0;36mAbstractFileSystem.info\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirectory\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 615\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: sagemaker-us-east-1-821908415262/fraud-detect-demo/processing_jobs/test_data/test.csv"
     ]
    }
   ],
   "source": [
    "# Fetch test data to run predictions with the endpoint\n",
    "test_df = pd.read_csv(f\"{processing_output_uri}/test_data/test.csv\")\n",
    "\n",
    "# Create SageMaker Predictor from the deployed endpoint\n",
    "predictor = sagemaker.predictor.Predictor(endpoint_name, \n",
    "                                          sagemaker_session=sess,\n",
    "                                          serializer=CSVSerializer(),\n",
    "                                          deserializer=CSVDeserializer()\n",
    "                                         )\n",
    "# Test endpoint with payload of 5 samples\n",
    "payload = test_df.drop([\"fraud\"], axis=1).iloc[:5]\n",
    "result = predictor.predict(payload.values)\n",
    "prediction_df = pd.DataFrame()\n",
    "prediction_df[\"Prediction\"] = result\n",
    "prediction_df[\"Label\"] = test_df[\"fraud\"].iloc[:5].values\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8574367c-6d86-4d02-bfd4-ae76bf487145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
